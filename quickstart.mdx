---
title: Quickstart
description: "Get started with Ladder AI: fine-tune LLMs and boost small models in minutes."
---

This quickstart demonstrates how to fine-tune an LLM using Ladder AI on a graph problem, either by loading an existing dataset or generating a new one from scratch.

---

## 1. Install Requirements

```sh
pip install ladder-ai trl
```

---

## 2. Prepare Environment

Create a `.env` file in your project root with your HuggingFace and OpenAI API keys:

```python
import os
os.environ["HF_TOKEN"] = os.environ.get("HF_TOKEN")
os.environ["OPENAI_API_KEY"] = os.environ.get("OPENAI_API_KEY")
```

---

## 3. Generate Dataset

```python
from ladder import setup_default_engines, load_basic_configs, generate_dataset
from ladder.finetuning import Ladder


problem_description = "Your Problem Description here "
config = load_basic_configs()
dataset = generate_dataset(problem_description=problem_description, config=config, dataset_len=10)
```

## 4. Ladder

```python

## VLadder Dataset
vladder_dataset = dataset.to_vladder()
vladder_dataset.apply_pattern("Answer: {}") # add your own pattern as u want
Qtrain, Qtest = vladder_dataset.split(0.8)

### Load Engines
llm_engine, verification_engine, difficulty_engine = setup_default_engines(config=config)

### Ladder
ladder = Ladder(vladder=Qtrain, config=config, verification_engine=verification_engine, reward_funcs=[])
ladder.finetune(save_locally=True)
```

---

---

## 5. Next Steps

- Explore the [Finetuning](./finetuning/ladder) and [Engines](./engines/llm_engine) docs for more options.
- Try different problem domains or custom reward functions.

Effortless LLM fine-tuningâ€”make small models smarter without data or supervision!
